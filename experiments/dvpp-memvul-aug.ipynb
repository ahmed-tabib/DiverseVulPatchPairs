{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8152991,"sourceType":"datasetVersion","datasetId":4822204}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -r *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-07T12:11:07.894211Z","iopub.execute_input":"2024-05-07T12:11:07.894562Z","iopub.status.idle":"2024-05-07T12:11:08.852143Z","shell.execute_reply.started":"2024-05-07T12:11:07.894531Z","shell.execute_reply":"2024-05-07T12:11:08.850954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"lr\": 1e-4,\n    \"bsz\": 1,\n    \"epochs\": 50,\n    \"max_sequence_length\": 1024,\n    \"cwe_list\": [\"CWE-119\", \"CWE-125\", \"CWE-787\"]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:11:08.854249Z","iopub.execute_input":"2024-05-07T12:11:08.854560Z","iopub.status.idle":"2024-05-07T12:11:08.859914Z","shell.execute_reply.started":"2024-05-07T12:11:08.854534Z","shell.execute_reply":"2024-05-07T12:11:08.858819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install wandb\n\n#import wandb\n#wandb.login(key='')\n#wandb.init(project='', config=config)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:11:08.861182Z","iopub.execute_input":"2024-05-07T12:11:08.861444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the basic performance metrics\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ndef calculate_perf_metrics(all_labels, all_predictions):\n    # evaluation scores\n    cm = confusion_matrix(all_labels, all_predictions)\n    accuracy = accuracy_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions)\n    recall = recall_score(all_labels, all_predictions)\n    f1 = f1_score(all_labels, all_predictions)\n\n    # Calculate false positive rate (FPR) and false negative rate (FNR)\n    tn, fp, fn, tp = cm.ravel()\n    fpr = fp / (fp + tn)\n    fnr = fn / (fn + tp)\n\n    return {\"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1-score\": f1,\n            \"FPR\": fpr,\n            \"FNR\": fnr\n           }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# the SPTs here are taken directly from the paper titled:\n# \"Limits of Machine Learning for Automatic Vulnerability Detection\"\n# https://arxiv.org/pdf/2306.17193.pdf (see table 1 for definitions of what SPTs do)\n# https://github.com/niklasrisse/LimitsOfML4Vuln\n\n# some transformations we will avoid are: tf_5, tf_9, tf_10, tf_13 (insertion/deletion of comment)\n# this is done because we remove all comments out of our code samples, since comments in patched\n# versions of a function most likely contain information about the fix, and may help the model\n# distinguish the vulnerable & patched versions without \"reading\" the actual source code.\n\n!wget -q https://raw.githubusercontent.com/ahmed-tabib/SPT/main/spt.py\n!mv transformations.py spt.py\n\nimport spt\nSPT_BLACKLIST = [spt.tf_5, spt.tf_10]\nSPT_WHITELIST = [spt.tf_1, spt.tf_2, spt.tf_3, spt.tf_4, spt.tf_6, spt.tf_7, spt.tf_8, spt.tf_9, spt.tf_12]\n\n\nimport random\n\n# This function applies a random set of SPTs, N times on a code sample, returning N samples\ndef apply_random_spts(code=\"\", spts=SPT_WHITELIST, spts_per_sample=2, num_of_samples=1):\n    try:\n        if spts_per_sample > len(spts) or code == \"\":\n            return []\n\n        result = []\n        for _ in range(num_of_samples):\n            tf_set = random.sample(spts, k=spts_per_sample)\n            c = code\n            for tf in tf_set:\n                c = tf(c)\n            result.append(c)\n\n        return result\n    except:\n        return [None]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, tokenizer, test_dataloader, max_steps=None):\n    all_labels = []\n    all_predictions = []\n\n    model.eval()\n    with torch.no_grad():\n        for step, samples in enumerate(test_dataloader):\n        \n            if max_steps:\n                if step >= max_steps:\n                    break\n            \n            # take each sample and set its label\n            inputs = samples['vuln'] + samples['patch']\n            labels = [1] * len(samples['vuln']) + [0] * len(samples['patch'])\n            \n            for i in range(len(inputs)):\n                inputs[i] = apply_random_spts(inputs[i], spts=SPT_WHITELIST, spts_per_sample=2, num_of_samples=1)[0]\n            \n            if None in inputs:\n                continue\n            \n            # tokenize and pad all to same length\n            tokenizer_output = tokenizer(inputs,\n                                                 return_tensors='pt',\n                                                 padding='longest',\n                                                 truncation=True,\n                                                 max_length=config[\"max_sequence_length\"])\n\n            input_ids = tokenizer_output['input_ids'].to(device)\n            attention_mask = tokenizer_output['attention_mask'].to(device)\n\n            # get model predictions\n            outputs = torch.tensor([])\n\n            for i, a in zip(input_ids, attention_mask):\n                outputs = torch.cat( (outputs, model(i.unsqueeze(0), a.unsqueeze(0)).cpu()) )\n\n            all_labels += labels\n            all_predictions += torch.min(outputs, dim=1).indices.tolist()\n\n    metrics = calculate_perf_metrics(all_labels, all_predictions)\n    return metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pandas as pd\n\nclass DiverseVPPDataset(Dataset):\n    def __init__(self, path):\n        self.df = pd.read_csv(path)\n        self.df.drop(columns=self.df.columns[0], axis=1, inplace=True)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        return {\"vuln\": self.df.iloc[idx].loc[\"vuln\"],\n                \"patch\": self.df.iloc[idx].loc[\"patch\"]}\n    \n    def get_df(self):\n        return self.df\n    \n    def set_df(self, df):\n        self.df = df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler\nfrom torch.utils.data.dataset import random_split\n\ndset = DiverseVPPDataset('/kaggle/input/diversevulpatchpairs/DiverseVulPatchPairs.csv')\n\ndf = dset.get_df().drop_duplicates(subset=['patch_hash'], keep=False)\ndf = df[df['cwe'].map(lambda e: any([(cwe in e) for cwe in config['cwe_list']]))]\ndf = df[df['patch'].map(lambda e: '{' in e and '}' in e)]\ndf = df[df['vuln'].map(lambda e: '{' in e and '}' in e)]\n    \ndset.set_df(df)\n\ntrain_dset, test_dset = random_split(dset, [0.85, 0.15])\n\nprint(\"Training set size: {} pairs, {} functions total.\".format(len(train_dset), len(train_dset)*2))\nprint(\"Testing set size:  {} pairs, {} functions total.\".format(len(test_dset), len(test_dset)*2))\nprint(\"Full set size:     {} pairs, {} functions total.\".format(len(dset), len(dset)*2))\n\ntrain_dataloader = DataLoader(dataset=train_dset,\n                              batch_size=config[\"bsz\"],\n                              shuffle=True,\n                              drop_last=False,\n                              num_workers=2)\n\ntest_dataloader = DataLoader(dataset=test_dset,\n                              batch_size=config[\"bsz\"],\n                              shuffle=False,\n                              drop_last=False,\n                              num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\n\ncheckpoint = \"Salesforce/codet5p-110m-embedding\"\n\ncodet5p_tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\ncodet5p_model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass VulnCodeT5(nn.Module):\n    \n    def __init__(self, codet5p):\n        super(VulnCodeT5, self).__init__()\n        \n        # codet5+ does its own embedding and normalization\n        # we want to customize this, so we only take the encoder\n        self.encoder = codet5p.encoder\n        \n        self.cls = nn.Sequential(\n            nn.Dropout(p=0.1),\n            nn.Linear(768, 3072),\n            nn.Tanh(),\n            nn.Dropout(p=0.1),\n            nn.Linear(3072, 3072),\n            nn.Linear(3072, 2)\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        encoder_outputs = self.encoder(input_ids, attention_mask)\n        output = self.cls(encoder_outputs.last_hidden_state[:, 0, :])\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Training loop","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = VulnCodeT5(codet5p_model)\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n\ncriterion = nn.BCEWithLogitsLoss()\n\n#print(eval_model(model, codet5p_tokenizer, test_dataloader, max_steps=250))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(config[\"epochs\"]):\n    model.train()\n    \n    all_labels_train = []\n    all_predictions_train = []\n    \n    for samples in train_dataloader:\n        # take each sample, augment it, and set its label\n        inputs = samples['vuln'] + samples['patch']\n        \n        labels = [1] * len(samples['vuln']) + [0] * len(samples['patch'])\n        \n        for i in range(len(inputs)):\n            inputs[i] = apply_random_spts(inputs[i], spts=SPT_WHITELIST, spts_per_sample=2, num_of_samples=1)[0]\n        \n        if None in inputs:\n            continue\n        \n        # tokenize and pad all to same length\n        tokenizer_output = codet5p_tokenizer(inputs,\n                                             return_tensors='pt',\n                                             padding='longest',\n                                             truncation=True,\n                                             max_length=config[\"max_sequence_length\"])\n        \n        input_ids = tokenizer_output['input_ids'].to(device)\n        attention_mask = tokenizer_output['attention_mask'].to(device)\n        labels = [[l, -1*(l-1)] for l in labels]\n        labels = torch.FloatTensor(labels)\n        \n        # forward pass\n        output = model(input_ids, attention_mask)\n            \n        # calc loss\n        loss = criterion(output.cpu(), labels)\n            \n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        #wandb.log({'epoch': epoch, 'loss': loss.item()})\n        \n        all_labels_train += torch.min(labels, dim=1).indices.tolist()\n        all_predictions_train += torch.min(output.cpu(), dim=1).indices.tolist()\n    \n    torch.save(model.state_dict(), './model.tmp')\n    \n    train_metrics = calculate_perf_metrics(all_labels_train, all_predictions_train)\n    test_metrics = eval_model(model, codet5p_tokenizer, test_dataloader, max_steps=250)\n    \n    train_metrics_wandb = {}\n    for k, v in train_metrics.items():\n        train_metrics_wandb[k+'_train'] = v\n    \n    test_metrics_wandb = {}\n    for k, v in test_metrics.items():\n        test_metrics_wandb[k+'_test'] = v\n        \n    #wandb.log(train_metrics_wandb)\n    #wandb.log(test_metrics_wandb)\n    \ntorch.save(model.state_dict(), './model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Eval","metadata":{}},{"cell_type":"code","source":"metrics = eval_model(model, codet5p_tokenizer, test_dataloader)\n#wandb.log({'final_metrics': metrics})\n#wandb.finish()\nprint(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}